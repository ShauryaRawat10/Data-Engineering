#### **Course Content**:
- Introduction AutoML - Databricks
- Training Regression model using AutoML
- Training Classification model using AutoML
- Registering and serving AutoML trained model

#### Databricks AutoML:
- AutoML helps you automatically apply ML to a dataset
- Without writing any code, entire machine learning pipeline starting from preprocessing data, applying / testing different models, registering model
- User specifies the dataset and prediction target
- AutoML takes care of complete ML workflow including how you want missing values to be imputed by configuration

#### Databricks Auto-ML training:
- Prepare data for training (Preprocessing and transformation)
- Run trials to tune and evaluate multiple models from specified frameworks
- Provides a python notebook for each trial run - users can review and tweak code
- Calculates summary statistics on data and make this available in a notebook

#### Supported models in AutoML
- Regression : To predict continuous values
  - Decision trees, Random Forest, Linear Regression with SGD, XGBoost, LightGBM
- Classification : To classify the data into categories
  - Decision trees, Random Forest, Logistic Regression, XGBoost, LightGBM
- Forecasting : To get time series forecast
  - Prophet, Auto-Arima frameworks

#### AutoML Features:
- Missing Value Imputation
  - mean, median and constant
- Large data sampling
  - to do data exploration and analysis, distribute data in nodes
- Imbalanced data detection
  - If data is skewed, then it will auto-balance the data for better categorization model
- Shapely values for model explanability
  - Importace of different features that you used to train your model, Shap values are avg expected marginal contribution of one feature after all possible combination have been considered
- Semantic type annotations
  - Give hints to AutoML
- Feature store integrations
  - Feature table in databricks


#### **Databricks ML environment**
- Catalog
- Machine Learning
  - Playground
  - Experiments
  - Features
  - Models
  - Serving

#### **Configuration of AutoML Experiment**
- Experiment
  - Select Impute With - Int/Double Data Type:
    - Auto
    - Mean
    - Median
    - Most Frequent
    - Constant
  - Select Impute With - String Data Type:
    - Most Frequent
    - Constant
  - Evaluation Merics:
    - R-squared (default)
    - MAE (mean absolute error)
    - MSE (mean squared error)
    - RMSE (root mean squared error)
  - Training frameworks:
    - lightgbm
    - sklearn
    - xgboost
  - Timeout
    - Default - 120 minute, (Pick different models from diffrent frameworks and perform hyper-parameter tunning using those models to find the best model for you)
  - Time column for training/validation/testing split
  - Intermediate data storage location
    - MLflow Artifact
    - DBFS Directory


#### ** Steps of AutoML operation**
- Run 01
  - Data Exploration Notebook : Training data Storage and Analysis
- Subsequent Run
  - New model executed and their Runs have been logged to Mileage-Prediction Experiment
    - random_forest_regressor and xgboost, lightgbm model have been trained
    - Analyze models using Notebooks available 
- Warning
  - High Correlation Columns: Low severity, It gave this warning as the data is highly correlated columns makes regression model less robust (AutoML did not do anything)
  - Categorical semantic type detected for columns: Medium severity, Columns expected to be numeric were found categorical (AutoML did this)
- Final Evaluation to select model
  - Runs of models will be sorted based on R-Square Score. Choose one with high R-Square.














