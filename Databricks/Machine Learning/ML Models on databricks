Overview of Databricks:

Databricks: 
- An enterprise software company founded by the creators of Apache spark. The company has also created Delta Lake, MLFlow, Koalas, and all open
source projects that span data engineering, data science, and machine learning
- A cloud native platform for big data processing, machine learning, and analytics built using Data Lakehouse architecture

- Databricks provides unified set of tools for enterprise-grade solutions
- Tools for building, deploying, sharing and maintaining these solutions
- Is clould native - manages and deploys cloud infrastructure on your behalf
- Integrates with cloud storage and security in your cloud
- Available on AWS, Azure, GCP

Databricks Core tools:
1. Apache Spark for big data processing
2. Delta Lakes to allow ACID transactions, versioning on huge datasets (metadata on top of data lets you do ACID)
3. SQL engine to run queries and build dashboards
4. Popular ML tools for traditional and deep learning models

Databricks provides the resources and tools needed to build and maintain Spark-based applications and also includes features to simplify business
analytics and build ML pipelines


Databricks Data Analytics platform  (3 personas/env)
1. Databricks SQL: Platform for analysts to run SQL queries on data, create visualizations, share dashboards
2. Databricks Data Science and Engineering: Interactive workspace for collaboration between data engineers, data science, and ML engineers to generate 
   insights using spark
3. Databricks ML: Integrated end-to-end machine learning environment with managed services for ML workflow


Databricks ML Runtime:
- Automates the creation of cluster optimized for Machine Learning
- Includes popular ML libraries:
  -> scikit-learn
  -> XGBoost
  -> SparkML
  -> Tensorflow
  -> PyTorch
- For training huge amount of data, includes support for distributed training libraries such as Horovod
- Include tools to automate the model development process
- Performs hyperparameter tuning to find the best model


Automate Machine Learning:
- AutoML: Automatically creates, tunes and evaluates model
- Managed ML Flow: Manages end-to-end model lifecycle, including tracking experiment runs, deploying, registering
                   and sharing model
- Hyperopt: Uses the sparktrials class to simplify hyperparameter tuning by automating and distributing model tuning runs


End to End Machine Learning Environment:

  Data Preparation ---> Feature Store  --> Model Training  --->   Models ---->   Production
  (Data Sources)        (Feature Store)    (AutoML)                              (Batch/Streaming Inference)
  (Delta Tables)                           (Notebooks)                           (Online Serving endpoints)
                                           (Experiments)

  Data Preparation: Use Spark or native programming language libraries to connect to data sources
                    Delta tables offers transaction support, version control, revision history for huge datasets
  Feature Store: Feature table in feature store allows you to store processed features for model training and inference
  Model training: Model training can be performed using custom code or with AutoML
                  Track parameters and model using experiments with MLflow tracking
  Models: Explore, register and serve model using model registry
  Production: Deploy models to production and perform inference on batch as well as streaming data


MLFlow:
Open source platform for managing end-to-end machine learning lifecycle which includes model tracking, model registry
model serving and inference.

MLFlow Components:
1. Model tracking
2. Models
3. Projects
4. Model registry
5. Model serving

Model tracking:
- API and UI for logging parameters, code versions, metrics and output files
- Tracking lets you log and query experiments
- Supported technologies and languages include Python, REST, R API and Java API





















































