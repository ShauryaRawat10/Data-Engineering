{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98a62ab-8c26-4ec6-833d-71bfab0060ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "Version History :\n",
    "Created by - Shaurya Rawat\n",
    "```\n",
    "\n",
    "</br>\n",
    "\n",
    "## Identity Conversion Utility\n",
    "- Backup Delta Files: Copying EDW table files from Current_Location to Old_Location \n",
    "- Creating unmanaged table for Old_Location\n",
    "- Converting Identity Column of current table from <b>GENERATED ALWAYS AS IDENTITY</b> to <b> GENERATED BY DEFAULT AS IDENTITY</b>\n",
    "- Resyncing Identity Column\n",
    "\n",
    "<i><h20> This code uses ipy_widgets for following reason: There is a known issue where a widget state may not properly clear after pressing Run All, even after clearing or removing the widget in code. If this happens, you will see a discrepancy between the widget’s visual state and its printed state. Re-running the cells individually may bypass this issue. To avoid this issue entirely, Databricks recommends that you use ipywidgets.</h20> </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ae967fa-6db5-4211-bd88-2af6877ebadc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Parameters"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36008f8e06ed406abf16cd82011ec8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Invoice_Payment_Status_Dim', description='Current Table:', layout=Layout(width='400px'), style=Des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7060d56133c9431a8781c82bd65fb205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Invoice_Payment_Status_Dim_Old', description='Backup Table:', layout=Layout(width='400px'), style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7482b8f30fbe4d6b9a56f2ed2317ce26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='datalake', description='Container:', layout=Layout(width='400px'), style=DescriptionStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de9dc331e9a4278b7099e959eda5cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='EDW', description='Zone:', layout=Layout(width='400px'), style=DescriptionStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "button = widgets.Button(description=\"Load dataframe sample\")\n",
    "# Set the following parameters for backup location from Folder1 to Folder2\n",
    "\n",
    "ipyw_table_name_current = widgets.Text(\n",
    "    value='Invoice_Payment_Status_Dim', \n",
    "    description='Current Table:',     \n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '150px'}\n",
    "    )\n",
    "\n",
    "ipyw_table_name_backup = widgets.Text(\n",
    "    value= ipyw_table_name_current.value + '_Old',\n",
    "    description='Backup Table:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Static values for the notebook\n",
    "ipyw_container_name = widgets.Text(\n",
    "    value='datalake',\n",
    "    description='Container:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "ipyw_adls_zone = widgets.Text(\n",
    "    value='EDW',\n",
    "    description='Zone:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Display widgets\n",
    "display(ipyw_table_name_current, ipyw_table_name_backup, ipyw_container_name, ipyw_adls_zone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8eb8d9-773c-405b-8933-cbfcfcfd9b3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating Backup Location"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped EDW External Table: Invoice_Payment_Status_Dim\nMoving files from abfss://datalake@[REDACTED].dfs.core.windows.net/EDW/Invoice_Payment_Status_Dim/Internal/ to abfss://datalake@[REDACTED].dfs.core.windows.net/EDW/Invoice_Payment_Status_Dim_Old/Internal/\nAll files processed...\nNumber of files processed: 9\n"
     ]
    }
   ],
   "source": [
    "# Creating Delta Lake backup folder in EDW\n",
    "\n",
    "# Access the values from ipywidgets\n",
    "p_table_name_current = ipyw_table_name_current.value\n",
    "p_table_name_backup = ipyw_table_name_backup.value\n",
    "p_container_name = ipyw_container_name.value\n",
    "p_adls_zone = ipyw_adls_zone.value\n",
    "\n",
    "storage_account_name = dbutils.secrets.get(scope = \"kv-edw-scope\", key = \"ADLS-StorageAccount\")\n",
    "\n",
    "Current_query_schema = f\"Show create table edw.{p_table_name_current}\"\n",
    "EDW_table_schema = spark.sql(Current_query_schema).collect()[0][0]\n",
    "\n",
    "source_path = f\"abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_current}/Internal/\"\n",
    "destination_path = f\"abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_backup}/Internal/\"\n",
    "\n",
    "try:\n",
    "    Drop_Current_Table = f\"Drop table edw.{p_table_name_current}\"\n",
    "    # Drop existing EDW table \n",
    "    spark.sql(Drop_Current_Table)\n",
    "    print(f\"Dropped EDW External Table: {p_table_name_current}\")\n",
    "except Exception as e:\n",
    "    print(\"Error dropping table\", e)\n",
    "\n",
    "try:\n",
    "    print(f\"Moving files from {source_path} to {destination_path}\")\n",
    "    dbutils.fs.mv(source_path, destination_path, True)\n",
    "    print(\"All files processed...\")\n",
    "    print(\"Number of files processed: \" + str(len(dbutils.fs.ls(destination_path) )) )\n",
    "except:\n",
    "    print(\"Error processing files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479818d7-26ce-4171-a8a9-95b1855ae54a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create external table for backup location"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nCREATE TABLE IF NOT EXISTS edw.`Invoice_Payment_Status_Dim_Old`\nUSING DELTA \nLOCATION 'abfss://datalake@[REDACTED].dfs.core.windows.net/EDW/Invoice_Payment_Status_Dim_Old/Internal'\n\nTable Invoice_Payment_Status_Dim_Old created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS edw.`{p_table_name_backup}`\n",
    "USING DELTA \n",
    "LOCATION 'abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_backup}/Internal'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(query)\n",
    "    print(query)\n",
    "    print(f\"Table {p_table_name_backup} created successfully or already exists.\")\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    print(f\"Table creation failed due to: {error_message}\")\n",
    "\n",
    "# displayHTML(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "065ef874-98aa-41b5-97d8-e93ce3efbf58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Regenerate EDW table schema - New Identity"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE sdev.edw.Invoice_Payment_Status_Dim (\n  Invoice_Payment_Status_Dim_Id BIGINT GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),\n  Rec_Process_Log_Id BIGINT NOT NULL,\n  Rec_Src_Application_Id INT NOT NULL,\n  Rec_Src_Key VARCHAR(150) NOT NULL,\n  Rec_Src_Create_Date TIMESTAMP,\n  Rec_Src_Create_User STRING,\n  Rec_Src_Update_Date TIMESTAMP,\n  Rec_Src_Update_User STRING,\n  Rec_EDW_Hash VARCHAR(255),\n  Rec_EDW_Create_Date TIMESTAMP NOT NULL,\n  Rec_EDW_Update_Date TIMESTAMP NOT NULL,\n  Invoice_Payment_Status_Code INT NOT NULL,\n  Invoice_Payment_Status VARCHAR(100) NOT NULL)\nUSING delta\nLOCATION 'abfss://datalake@[REDACTED].dfs.core.windows.net/EDW/Invoice_Payment_Status_Dim/Internal'\nTBLPROPERTIES (\n  'delta.enableDeletionVectors' = 'true',\n  'delta.feature.deletionVectors' = 'supported',\n  'delta.feature.identityColumns' = 'supported',\n  'delta.feature.invariants' = 'supported',\n  'delta.minReaderVersion' = '3',\n  'delta.minWriterVersion' = '7')\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    Sync_identity_query = f\"ALTER TABLE edw.{p_table_name_current} ALTER COLUMN {p_table_name_current}_Id SYNC IDENTITY\"\n",
    "    insert_query = f\"INSERT INTO edw.{p_table_name_current} SELECT * FROM edw.{p_table_name_backup}\";\n",
    "    modified_schema = EDW_table_schema.replace(f\"GENERATED ALWAYS AS IDENTITY\", f\"GENERATED BY DEFAULT AS IDENTITY\")\n",
    "    \n",
    "    # Delete existing directory in ADLS for EDW table to avoid sync issues\n",
    "    dbutils.fs.rm(source_path, recurse=True)\n",
    "\n",
    "    # Recreate EDW table with new Identity features\n",
    "    spark.sql(modified_schema)\n",
    "\n",
    "    # Insert data into EDW table\n",
    "    spark.sql(insert_query)\n",
    "\n",
    "    # Syncing Identity Value\n",
    "    spark.sql(Sync_identity_query)\n",
    "\n",
    "    print(modified_schema)\n",
    "except Exception as e:\n",
    "    error_message = str(e)\n",
    "    print(f\"Failed due to: {error_message}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5780afbb-2b55-43c9-a528-888c6c0a9e3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Both tables have the same data.\n"
     ]
    }
   ],
   "source": [
    "Backup_table_validation = 0\n",
    "\n",
    "result_backup_minus_current = spark.sql(f\"\"\"\n",
    "    SELECT * FROM edw.`{p_table_name_backup}`\n",
    "    MINUS\n",
    "    SELECT * FROM edw.`{p_table_name_current}`\n",
    "\"\"\")\n",
    "\n",
    "if result_backup_minus_current.count() == 0:\n",
    "    Backup_table_validation = 1\n",
    "    print(\"Success: Both tables have the same data.\")\n",
    "else:\n",
    "    print(\"Error: Data mismatch found.\")\n",
    "    result_backup_minus_current.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1821759807452834,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "IdentityConversionUtility_ipy_widget",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}