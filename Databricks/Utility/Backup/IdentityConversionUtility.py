# Databricks notebook source
# MAGIC %md
# MAGIC `
# MAGIC Version History :
# MAGIC Created by - Shaurya Rawat
# MAGIC `
# MAGIC
# MAGIC </br>
# MAGIC
# MAGIC ## Identity Conversion Utility
# MAGIC - Backup Delta Files: Copying EDW table files from Current_Location to Old_Location 
# MAGIC - Creating unmanaged table for Old_Location
# MAGIC - Converting Identity Column of current table from <b>GENERATED ALWAYS AS IDENTITY</b> to <b> GENERATED BY DEFAULT AS IDENTITY</b>
# MAGIC - Resyncing Identity Column

# COMMAND ----------

# DBTITLE 1,Clear Parameters
dbutils.widgets.removeAll()

# COMMAND ----------

# DBTITLE 1,Set Parameters
# Set the following parameters for backup location from Folder1 to Folder2
p_table_name_current = 'Invoice_Payment_Status_Dim'
p_table_name_backup = 'Invoice_Payment_Status_Dim_Old'

# Static values for the notebook
p_container_name= 'datalake'
p_adls_zone = 'EDW'

dbutils.widgets.text("p_table_name_current",p_table_name_current) 
dbutils.widgets.text("p_table_name_backup",p_table_name_backup) 
dbutils.widgets.text("p_container_name",p_container_name) 
dbutils.widgets.text("p_adls_zone",p_adls_zone) 

# COMMAND ----------

# DBTITLE 1,Creating Backup Location
# Creating Delta Lake backup folder in EDW
p_table_name_current = dbutils.widgets.get("p_table_name_current")
p_table_name_backup = dbutils.widgets.get("p_table_name_backup")
p_container_name = dbutils.widgets.get("p_container_name")
p_adls_zone = dbutils.widgets.get("p_adls_zone")
storage_account_name = dbutils.secrets.get(scope = "kv-edw-scope", key = "ADLS-StorageAccount")

Current_query_schema = f"Show create table edw.{p_table_name_current}"
EDW_table_schema = spark.sql(Current_query_schema).collect()[0][0]

source_path = f"abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_current}/Internal/"
destination_path = f"abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_backup}/Internal/"

try:
    Drop_Current_Table = f"Drop table edw.{p_table_name_current}"
    # Drop existing EDW table 
    spark.sql(Drop_Current_Table)
    print(f"Dropped EDW External Table: {p_table_name_current}")
except Exception as e:
    print("Error dropping table", e)

try:
    print(f"Moving files from {source_path} to {destination_path}")
    dbutils.fs.mv(source_path, destination_path, True)
    print("All files processed...")
    print("Number of files processed: " + str(len(dbutils.fs.ls(destination_path) )) )
except:
    print("Error processing files")

# COMMAND ----------

# DBTITLE 1,Create external table for backup location
query = f"""
CREATE TABLE IF NOT EXISTS edw.`{p_table_name_backup}`
USING DELTA 
LOCATION 'abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_backup}/Internal'
"""

try:
    spark.sql(query)
    print(query)
    print(f"Table {p_table_name_backup} created successfully or already exists.")
except Exception as e:
    error_message = str(e)
    print(f"Table creation failed due to: {error_message}")

# displayHTML(query) 

# COMMAND ----------

# DBTITLE 1,Regenerate EDW table schema - New Identity

try:
    Sync_identity_query = f"ALTER TABLE edw.{p_table_name_current} ALTER COLUMN {p_table_name_current}_Id SYNC IDENTITY"
    insert_query = f"INSERT INTO edw.{p_table_name_current} SELECT * FROM edw.{p_table_name_backup}";
    modified_schema = EDW_table_schema.replace(f"GENERATED ALWAYS AS IDENTITY", f"GENERATED BY DEFAULT AS IDENTITY")
    
    # Recreate EDW table with new Identity features
    spark.sql(modified_schema)

    # Insert data into EDW table
    spark.sql(insert_query)

    # Syncing Identity Value
    spark.sql(Sync_identity_query)

    print(modified_schema)
except Exception as e:
    error_message = str(e)
    print(f"Failed due to: {error_message}")




# COMMAND ----------

# DBTITLE 1,Validation
Backup_table_validation = 0

result_backup_minus_current = spark.sql(f"""
    SELECT * FROM edw.`{p_table_name_backup}`
    MINUS
    SELECT * FROM edw.`{p_table_name_current}`
""")

if result_backup_minus_current.count() == 0:
    Backup_table_validation = 1
    print("Success: Both tables have the same data.")
else:
    print("Error: Data mismatch found.")
    result_backup_minus_current.show()
