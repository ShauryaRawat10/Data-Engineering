# Databricks notebook source
# MAGIC %md
# MAGIC ```
# MAGIC Version History :
# MAGIC Created by - Shaurya Rawat
# MAGIC ```
# MAGIC
# MAGIC </br>
# MAGIC
# MAGIC ## Identity Conversion Utility
# MAGIC - Backup Delta Files: Copying EDW table files from Current_Location to Old_Location 
# MAGIC - Creating unmanaged table for Old_Location
# MAGIC - Converting Identity Column of current table from <b>GENERATED ALWAYS AS IDENTITY</b> to <b> GENERATED BY DEFAULT AS IDENTITY</b>
# MAGIC - Resyncing Identity Column
# MAGIC
# MAGIC <i><h20> This code uses ipy_widgets for following reason: There is a known issue where a widget state may not properly clear after pressing Run All, even after clearing or removing the widget in code. If this happens, you will see a discrepancy between the widgetâ€™s visual state and its printed state. Re-running the cells individually may bypass this issue. To avoid this issue entirely, Databricks recommends that you use ipywidgets.</h20> </i>

# COMMAND ----------

# DBTITLE 1,Set Parameters
import ipywidgets as widgets
from IPython.display import display

button = widgets.Button(description="Load dataframe sample")
# Set the following parameters for backup location from Folder1 to Folder2

ipyw_table_name_current = widgets.Text(
    value='Invoice_Payment_Status_Dim', 
    description='Current Table:',     
    layout=widgets.Layout(width='400px'),
    style={'description_width': '150px'}
    )

ipyw_table_name_backup = widgets.Text(
    value= ipyw_table_name_current.value + '_Old',
    description='Backup Table:',
    layout=widgets.Layout(width='400px'),
    style={'description_width': '150px'}
)

# Static values for the notebook
ipyw_container_name = widgets.Text(
    value='datalake',
    description='Container:',
    layout=widgets.Layout(width='400px'),
    style={'description_width': '150px'}
)

ipyw_adls_zone = widgets.Text(
    value='EDW',
    description='Zone:',
    layout=widgets.Layout(width='400px'),
    style={'description_width': '150px'}
)

# Display widgets
display(ipyw_table_name_current, ipyw_table_name_backup, ipyw_container_name, ipyw_adls_zone)


# COMMAND ----------

# DBTITLE 1,Creating Backup Location
# Creating Delta Lake backup folder in EDW

# Access the values from ipywidgets
p_table_name_current = ipyw_table_name_current.value
p_table_name_backup = ipyw_table_name_backup.value
p_container_name = ipyw_container_name.value
p_adls_zone = ipyw_adls_zone.value

storage_account_name = dbutils.secrets.get(scope = "kv-edw-scope", key = "ADLS-StorageAccount")

Current_query_schema = f"Show create table edw.{p_table_name_current}"
EDW_table_schema = spark.sql(Current_query_schema).collect()[0][0]

source_path = f"abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_current}/Internal/"
destination_path = f"abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_backup}/Internal/"

try:
    Drop_Current_Table = f"Drop table edw.{p_table_name_current}"
    # Drop existing EDW table 
    spark.sql(Drop_Current_Table)
    print(f"Dropped EDW External Table: {p_table_name_current}")
except Exception as e:
    print("Error dropping table", e)

try:
    print(f"Moving files from {source_path} to {destination_path}")
    dbutils.fs.mv(source_path, destination_path, True)
    print("All files processed...")
    print("Number of files processed: " + str(len(dbutils.fs.ls(destination_path) )) )
except:
    print("Error processing files")

# COMMAND ----------

# DBTITLE 1,Create external table for backup location
query = f"""
CREATE TABLE IF NOT EXISTS edw.`{p_table_name_backup}`
USING DELTA 
LOCATION 'abfss://{p_container_name}@{storage_account_name}.dfs.core.windows.net/{p_adls_zone}/{p_table_name_backup}/Internal'
"""

try:
    spark.sql(query)
    print(query)
    print(f"Table {p_table_name_backup} created successfully or already exists.")
except Exception as e:
    error_message = str(e)
    print(f"Table creation failed due to: {error_message}")

# displayHTML(query) 

# COMMAND ----------

# DBTITLE 1,Regenerate EDW table schema - New Identity

try:
    Sync_identity_query = f"ALTER TABLE edw.{p_table_name_current} ALTER COLUMN {p_table_name_current}_Id SYNC IDENTITY"
    insert_query = f"INSERT INTO edw.{p_table_name_current} SELECT * FROM edw.{p_table_name_backup}";
    modified_schema = EDW_table_schema.replace(f"GENERATED ALWAYS AS IDENTITY", f"GENERATED BY DEFAULT AS IDENTITY")
    
    # Delete existing directory in ADLS for EDW table to avoid sync issues
    dbutils.fs.rm(source_path, recurse=True)

    # Recreate EDW table with new Identity features
    spark.sql(modified_schema)

    # Insert data into EDW table
    spark.sql(insert_query)

    # Syncing Identity Value
    spark.sql(Sync_identity_query)

    print(modified_schema)
except Exception as e:
    error_message = str(e)
    print(f"Failed due to: {error_message}")




# COMMAND ----------

# DBTITLE 1,Validation
Backup_table_validation = 0

result_backup_minus_current = spark.sql(f"""
    SELECT * FROM edw.`{p_table_name_backup}`
    MINUS
    SELECT * FROM edw.`{p_table_name_current}`
""")

if result_backup_minus_current.count() == 0:
    Backup_table_validation = 1
    print("Success: Both tables have the same data.")
else:
    print("Error: Data mismatch found.")
    result_backup_minus_current.show()