# Databricks notebook source
# DBTITLE 1,Get Required Libraries and Params
import json

# Set default job paarameters
dbutils.widgets.text("P_Job_Name", "Shaurya's Job Name", "Job Name")
dbutils.widgets.text("P_Workspace_Id", "Shaurya's Workspace Id", "Workspace ID")

# COMMAND ----------

# DBTITLE 1,Create table in DBR Catalog
# MAGIC %sql
# MAGIC Create table if not exists Shaurya.Notebook_Runs (
# MAGIC   Notebook_Runs_ID BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   Job_Run_Id STRING,
# MAGIC   Job_Name STRING,
# MAGIC   User STRING,
# MAGIC   Cluster_Id STRING,
# MAGIC   Workspace_Id STRING,
# MAGIC   Notebook_Path STRING
# MAGIC )

# COMMAND ----------

# DBTITLE 1,Insert Auditing data into table
# Get the context as JSON string
context_str = dbutils.notebook.entry_point.getDbutils().notebook().getContext().safeToJson()

# Parse the JSON string into a Python dictionary
context_dict = json.loads(context_str)

# Pretty-print the dictionary
beautify_json = json.dumps(context_dict, indent=4)

current_run_id = context_dict['attributes']['currentRunId']
user = context_dict['attributes']['user']
cluster_id = context_dict['attributes']['clusterId']
notebook_path = context_dict['attributes']['notebook_path']
job_name = dbutils.widgets.get("P_Job_Name")
workspace_id = dbutils.widgets.get("P_Workspace_Id")

sql_insert = f"""
INSERT INTO Shaurya.Notebook_Runs (
    Job_Run_Id,
    Job_Name,
    User,
    Cluster_Id,
    Workspace_Id,
    Notebook_Path
) VALUES (
    '{current_run_id}',
    '{job_name}',
    '{user}',
    '{cluster_id}',
    '{workspace_id}',
    '{notebook_path}'
);
"""

# Print the generated SQL query
spark.sql(sql_insert)

# COMMAND ----------

# MAGIC %sql
# MAGIC Select * from  Shaurya.Notebook_Runs 