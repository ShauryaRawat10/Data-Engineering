{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75e39fd2-77ac-41a4-8feb-3fabe630f83d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###IPL : Indian Premier Data League\n",
    "\n",
    "Code Components;\n",
    "1. Data Storage: Amazon s3\n",
    "2. Databricks\n",
    "    - Transformation (Spark)\n",
    "    - SQL Analytics: SQL\n",
    "    - Visualization \n",
    "\n",
    "\n",
    "####Ecosystem of Spark Core\n",
    "- Apache Spark Core: Executing every code that is passed to spark. It is heart of Spark\n",
    "- Spark SQL: For SQL query\n",
    "- Spark Streaming: For processing real time data\n",
    "- MLlib (Machine learning): ML on large data\n",
    "- GraphX (Graph): Graphical db \n",
    "\n",
    "####Apache Spark\n",
    "Apache spark is unified computing engine and a set of libraries for parallel data processing on computer clusters\n",
    "Basically framework which partitions data in distributed way on machine and combines output at the end\n",
    "1. Structured Streaming, Advanced Analytics, Libraries and Ecosystem\n",
    "2. Structured APIs: Datasets, DataFrames, SQL\n",
    "3. Low level APIs: RDDs, Distributed Variables\n",
    "\n",
    "####Architecture of Apache Spark\n",
    "Single machines do not have enough power and resources as if they were single computer. A cluster is needed.\n",
    "A cluster, is a group of computers, pools the resources of many machines together, giving us the ability to use all cumulative resources as if they were a single computer\n",
    "A group of machines alone is not powerful, you need framework to coordinate work accross them. Spark does just that, managing and coordinating the execution of tasks on data across a cluster of computers\n",
    "![Delta arch](/files/tables/Shaurya/Spark_architecture_1.png)\n",
    "\n",
    "Spark applications consist of 2 processes:\n",
    "1. Driver Process: \n",
    "> Runs the main() function\n",
    "> 1. Managing information about spark application\n",
    "> 2. Responding to users program or input\n",
    "> 3. Analyzing, distributing, and scheduling work across the executors\n",
    "2. Executor Process\n",
    "> 1. Executing code assigned to it by driver\n",
    "> 2. Reporting the state of computation on that executor back to driver node\n",
    "\n",
    "SparkSession\n",
    "You control your spark application through a driver process called SparkSession\n",
    "Default variable created when executed code, entry point\n",
    "\n",
    "Spark\n",
    "O/P: <pyspark.sql.session.SparkSession>\n",
    "\n",
    "#####Spark DataFrame:\n",
    "A dataframe is most common Structured API and simply represents a table of data with rows and columns\n",
    "This is also distributedly\n",
    "\n",
    "#####Transformations\n",
    "Business logic on data\n",
    "Creates logical plan and physical plan for execution\n",
    "Only executed after Action is made\n",
    "\n",
    "divsBy2 = myRange.where(\"number % 2 == 0\")\n",
    "divsBy2.count() -- Action\n",
    "\n",
    "######Databricks\n",
    "Software that supports Apache spark environment\n",
    "No need to install JVM, packages, configure env path on 1000nds of machines, scaling and networking. \n",
    "Databricks takes care of all these. Only focus on writing code. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9752d3-02e1-4765-8650-2116bb2a5b90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=6722499136019472#setting/sparkui/0701-093134-oxhmwta2/driver-6795448222560662075\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.34.17.135:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa1df169bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%python\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5a8b1c8-8563-442a-98a7-37c23701a5d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create Session\n",
    "spark = SparkSession.builder.appName(\"IPL Data Analysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d51155e-7a02-4fd2-bb94-3ea1bad97be4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=6722499136019472#setting/sparkui/0701-093134-oxhmwta2/driver-6795448222560662075\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.34.17.135:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3f8f7b2e00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b2bed2-a986-4efe-9bf7-da53f6184b54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe \n",
    "# It reads everything as String, inferSchema for detecting the correct datatype. It is also problem as it can read boolean to int and so on, create your own schema is best practice\n",
    "bal_by_ball_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"s3://ipl-data-analysis-project/Ball_By_Ball.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074873bf-a81c-44e7-8353-48185b91c578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, BooleanType, DateType, DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41650aa0-1992-457b-aae4-dbfb683ace6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ball_by_ball_schema = StructType([\n",
    "    StructField(\"Match_id\", IntegerType(), True),  # True means it can be null\n",
    "    StructField(\"over_id\", IntegerType(), True),\n",
    "    StructField(\"ball_id\", IntegerType(), True),\n",
    "    StructField(\"innings_no\", IntegerType(), True),\n",
    "    StructField(\"team_batting\", StringType(), True),\n",
    "    StructField(\"team_bowling\", StringType(), True),\n",
    "    StructField(\"striker_batting_position\", IntegerType(), True),\n",
    "    StructField(\"extra_type\", StringType(), True),\n",
    "    StructField(\"runs_scored\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"bowler_extras\", IntegerType(), True),\n",
    "    StructField(\"out_type\", StringType(), True),\n",
    "    StructField(\"caught\", BooleanType(), True),\n",
    "    StructField(\"bowled\", BooleanType(), True),\n",
    "    StructField(\"run_out\", BooleanType(), True),\n",
    "    StructField(\"lbw\", BooleanType(), True),\n",
    "    StructField(\"retired_hurt\", BooleanType(), True),\n",
    "    StructField(\"stumped\", BooleanType(), True),\n",
    "    StructField(\"caught_and_bowled\", BooleanType(), True),\n",
    "    StructField(\"hit_wicket\", BooleanType(), True),\n",
    "    StructField(\"obstructingfeild\", BooleanType(), True),\n",
    "    StructField(\"bowler_wicket\", BooleanType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"striker\", IntegerType(), True),\n",
    "    StructField(\"non_striker\", IntegerType(), True),\n",
    "    StructField(\"bowler\", IntegerType(), True),\n",
    "    StructField(\"player_out\", IntegerType(), True),\n",
    "    StructField(\"fielders\", IntegerType(), True),\n",
    "    StructField(\"striker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"strikersk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_match_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_match_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_sk\", IntegerType(), True),\n",
    "    StructField(\"playerout_match_sk\", IntegerType(), True),\n",
    "    StructField(\"battingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"bowlingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"keeper_catch\", BooleanType(), True),\n",
    "    StructField(\"player_out_sk\", IntegerType(), True),\n",
    "    StructField(\"matchdatesk\", DateType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc47132-a9fe-4117-a86a-a86a638aa0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ball_by_ball_df = spark.read.schema(ball_by_ball_schema).format(\"csv\").option(\"header\", \"true\").load(\"s3://ipl-data-analysis-project/Ball_By_Ball.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3b2a636-45d7-4253-ad6d-6eec786472f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+----------+------------+------------+------------------------+----------+-----------+----------+-----+-------+----+-------+-------+-------------+--------------+------+------+-------+----+------------+-------+-----------------+----------+----------------+-------------+----------+------+-------+-----------+------+----------+--------+----------------+---------+-------------------+-------------+----------------+----------+---------------+---------+------------------+--------------+--------------+------------+-------------+-----------+\n|Match_id|over_id|ball_id|innings_no|team_batting|team_bowling|striker_batting_position|extra_type|runs_scored|extra_runs|wides|legbyes|byes|noballs|penalty|bowler_extras|      out_type|caught|bowled|run_out| lbw|retired_hurt|stumped|caught_and_bowled|hit_wicket|obstructingfeild|bowler_wicket|match_date|season|striker|non_striker|bowler|player_out|fielders|striker_match_sk|strikersk|nonstriker_match_sk|nonstriker_sk|fielder_match_sk|fielder_sk|bowler_match_sk|bowler_sk|playerout_match_sk|battingteam_sk|bowlingteam_sk|keeper_catch|player_out_sk|matchdatesk|\n+--------+-------+-------+----------+------------+------------+------------------------+----------+-----------+----------+-----+-------+----+-------+-------+-------------+--------------+------+------+-------+----+------------+-------+-----------------+----------+----------------+-------------+----------+------+-------+-----------+------+----------+--------+----------------+---------+-------------------+-------------+----------------+----------+---------------+---------+------------------+--------------+--------------+------------+-------------+-----------+\n|  598028|     15|      6|         1|           5|           2|                       6| No Extras|          4|         0|    0|      0|   0|      0|      0|            0|Not Applicable|  NULL|  NULL|   NULL|NULL|        NULL|   NULL|             NULL|      NULL|            NULL|         NULL|      NULL|  2013|    277|        104|    83|      NULL|    NULL|           20336|      276|              20333|          103|              -1|        -1|          20343|       82|                -1|             4|             1|        NULL|            0|       NULL|\n|  598028|     14|      1|         1|           5|           2|                       5| No Extras|          1|         0|    0|      0|   0|      0|      0|            0|Not Applicable|  NULL|  NULL|   NULL|NULL|        NULL|   NULL|             NULL|      NULL|            NULL|         NULL|      NULL|  2013|    104|          6|   346|      NULL|    NULL|           20333|      103|              20328|            5|              -1|        -1|          20348|      345|                -1|             4|             1|        NULL|            0|       NULL|\n|  598028|     14|      2|         1|           5|           2|                       3| No Extras|          1|         0|    0|      0|   0|      0|      0|            0|Not Applicable|  NULL|  NULL|   NULL|NULL|        NULL|   NULL|             NULL|      NULL|            NULL|         NULL|      NULL|  2013|      6|        104|   346|      NULL|    NULL|           20328|        5|              20333|          103|              -1|        -1|          20348|      345|                -1|             4|             1|        NULL|            0|       NULL|\n|  598028|     14|      3|         1|           5|           2|                       5| No Extras|          1|         0|    0|      0|   0|      0|      0|            0|Not Applicable|  NULL|  NULL|   NULL|NULL|        NULL|   NULL|             NULL|      NULL|            NULL|         NULL|      NULL|  2013|    104|          6|   346|      NULL|    NULL|           20333|      103|              20328|            5|              -1|        -1|          20348|      345|                -1|             4|             1|        NULL|            0|       NULL|\n|  598028|     14|      4|         1|           5|           2|                       3| No Extras|          0|         0|    0|      0|   0|      0|      0|            0|Not Applicable|  NULL|  NULL|   NULL|NULL|        NULL|   NULL|             NULL|      NULL|            NULL|         NULL|      NULL|  2013|      6|        104|   346|      NULL|    NULL|           20328|        5|              20333|          103|              -1|        -1|          20348|      345|                -1|             4|             1|        NULL|            0|       NULL|\n+--------+-------+-------+----------+------------+------------+------------------------+----------+-----------+----------+-----+-------+----+-------+-------+-------------+--------------+------+------+-------+----+------------+-------+-----------------+----------+----------------+-------------+----------+------+-------+-----------+------+----------+--------+----------------+---------+-------------------+-------------+----------------+----------+---------------+---------+------------------+--------------+--------------+------------+-------------+-----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "ball_by_ball_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f139c21-4153-4eb8-ab63-df45b3a54cd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_schema = StructType([\n",
    "    StructField(\"match_sk\", IntegerType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"team1\", StringType(), True),\n",
    "    StructField(\"team2\", StringType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"venue_name\", StringType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"toss_winner\", StringType(), True),\n",
    "    StructField(\"match_winner\", StringType(), True),\n",
    "    StructField(\"toss_name\", StringType(), True),\n",
    "    StructField(\"win_type\", StringType(), True),\n",
    "    StructField(\"outcome_type\", StringType(), True),\n",
    "    StructField(\"manofmach\", StringType(), True),\n",
    "    StructField(\"win_margin\", IntegerType(), True),\n",
    "    StructField(\"country_id\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d537efda-66be-4465-b3ad-f373d9112119",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "match_df = spark.read.schema(match_schema).format(\"csv\").option(\"header\", \"true\").load(\"s3://ipl-data-analysis-project/Match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940d5762-ba00-404d-86a8-6e8703bf749d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_schema = StructType([\n",
    "    StructField(\"player_sk\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True)\n",
    "])\n",
    "player_df = spark.read.schema(player_schema).format(\"csv\").option(\"header\", \"true\").load(\"s3://ipl-data-analysis-project/Player.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9bc14d-66f0-439d-96d8-b762f49713cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_match_schema = StructType([\n",
    "    StructField(\"player_match_sk\", IntegerType(), True),\n",
    "    StructField(\"playermatch_key\", DecimalType(10, 0), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"role_desc\", StringType(), True),\n",
    "    StructField(\"player_team\", StringType(), True),\n",
    "    StructField(\"opposit_team\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"is_manofthematch\", BooleanType(), True),\n",
    "    StructField(\"age_as_on_match\", IntegerType(), True),\n",
    "    StructField(\"isplayers_team_won\", BooleanType(), True),\n",
    "    StructField(\"batting_status\", StringType(), True),\n",
    "    StructField(\"bowling_status\", StringType(), True),\n",
    "    StructField(\"player_captain\", StringType(), True),\n",
    "    StructField(\"opposit_captain\", StringType(), True),\n",
    "    StructField(\"player_keeper\", StringType(), True),\n",
    "    StructField(\"opposit_keeper\", StringType(), True)\n",
    "])\n",
    "player_match_df = spark.read.schema(player_match_schema).format(\"csv\").option(\"header\", \"true\").load(\"s3://ipl-data-analysis-project/Player_match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f89e66-0c1f-4335-926f-9c0dc30647e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_schema = StructType([\n",
    "    StructField(\"team_sk\", IntegerType(), True),\n",
    "    StructField(\"team_id\", IntegerType(), True),\n",
    "    StructField(\"team_name\", StringType(), True)\n",
    "])\n",
    "team_df = spark.read.schema(team_schema).format(\"csv\").option(\"header\", \"true\").load(\"s3://ipl-data-analysis-project/Team.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ecea65e-3820-4378-80b8-11f9a4772ec8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transformation - Structured API\n",
    "> Lazy evaluation : Until action is done, no actual processing of data.\n",
    "   Spark will wait until the very last moment to execute the graph of computation instructions\n",
    "```\n",
    "   SQL             --->\n",
    "   Dataframes      ---> Catalist Optimizer   ----> Physical Plan  (first create logical plan)\n",
    "   DataSets        --->\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1d071e-0c02-46fd-ac5e-1c2efddab1db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, sum, avg, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2035d6ed-872d-47e3-9be6-30cff0c2ce41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Filter to include only valid deliverables (excluding extras like wides and no balls for specific analysis)\n",
    "ball_by_ball_df = ball_by_ball_df.filter( (col(\"wides\") == 0) & (col(\"noballs\") == 0) )\n",
    "\n",
    "# Aggregation: Calculate the total and average runs scored in each match and inning\n",
    "total_and_avg_runs = ball_by_ball_df.groupBy(\"match_id\", \"innings_no\").agg(\n",
    "  sum(\"runs_scored\").alias(\"total_runs\"),\n",
    "  avg(\"runs_scored\").alias(\"average_runs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903214ee-53b1-453c-9014-01a25e08115c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IPL_Data_Analysis_Spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
