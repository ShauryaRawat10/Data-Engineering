![image](https://github.com/user-attachments/assets/49e17735-7bda-4588-83cf-b6c8ae0a5d3e)# Model to Automate Anomaly Detection

#### Overview
- Anomaly Detection Technique
  - STL decomposition
  - Classification and Regression trees
  - Clustering-based anomaly detection
  - Anomaly detection using Autoencoders
- Introduction to problem and dataset
- Exploratory data analysis and data cleaning
- Building a model for anomaly detection

<br>

#### STL Decomposition
- STL -Seasonal and Trendy decomposition using LOESS (LOESS is method for calculating non-linear relationship)
- This technique gives ability to split time series into 3 parts
  - Seasonal
  - Trend
  - Residue
 
![STLDecomposition](https://github.com/ShauryaRawat10/Data-Engineering/blob/67a641b360a09805a02fb3db75cc0783272574be/Machine%20Learning/Basics/Storage/STLTechnique.png)

Pros: Simple, robust and can handle lot of different situations
Cons: Rigid tweeking option, Threshold and Confidence interval are only thing to control

#### Classification and Regression techniques
- Utilizing the power of decision trees to identify Anomalies
- Types:
  - Supervised (labelled anomilous data points which would be difficult, causing issues with supervised)
  - Unsupervised
    - Isolisation Forest (Detect Outlier without labelled data)
<br>
<br>


#### Isolation Forest
- Built based on decision trees
- Based on fact that anomalies are few and different
- Esemble of binary trees and each tree is called Isolation tree
- Make partitions so that each data point is isolated

![isolation Trees](https://github.com/ShauryaRawat10/Data-Engineering/blob/f270e6c7623751b7085d28d002943bff5ed705f7/Machine%20Learning/Basics/Storage/IsolationTreesEx1.jpeg)

- When we do patitions, anomolous point is easier and faster to isolate than a normal data point


###### Isolation Forest Algorithm
- Training: Building a forest of Isolation trees (iTree)
- Take a sample of dataset and build iTrees until each point is isolated
  - Randomly select a feature
  - Randomly partition along the range
- During prediction, an "anomaly score" is assigned to each of the data points based on depth of the tree required to arrive that point
  - -1 to anomalies and +1 to normal points
- Pros: Introduce as many random variables or features as you like
- Cons: Growing number of features can impact computational performace

<br>
<br>

#### Clustering
- Clustering is an unsupervised ML technique of dividing a dataset into number of groups or clusters such that data points in the same cluster are similar to each other

#### Clustering based Anomaly detection
- Normal data points belong to large and dense clusters, while outliers belong to small and sparse clusters and do not belong to any cluster
- Data point is an anomaly if:
  - It does not belongs to any cluster
  - Large distance between data point and cluster
  - Belong to small or sparse cluster

#### K-means Clustering
- k-means is an unsupervised clustering algorithm designed to partition unlabeled data into certain number - denoted by 'K' - of distinct groupings

![KmeansClustering](https://github.com/ShauryaRawat10/Data-Engineering/blob/0a55f8c4dba9f1f717809bf92bda147a037e938c/Machine%20Learning/Basics/Storage/kmeansClustering.png)

- We calculate distance of each datav point from its centroid. Set distance threshold. Any data point outside the distance is anomaly
- Pros: Introduce as many random variables or features as you like
- Cons: Growing number of features can impact computational performace
- Cons: More hyper-parameters to tune, so there is chance of high model variance in performance

<br>
<br>

#### Anomaly detection using Autoencoders
- Auto-encoders are type of neural network in which input and output are identical. They're typically used for image denoising and dimensionality reduction

![Autoencoders](https://github.com/ShauryaRawat10/Data-Engineering/blob/6c6e82535dca7614db58305343a9e317ed752acc/Machine%20Learning/Basics/Storage/Auto-encoders1.png)

- Encoder: Encodes and compresses input into reduced dimension. Compressed data is distorted image of Original input
- Code: Code of network represents the compressed input which is fed to the decoder. Code is also known as bottle-neck
- Decoder: Decodes image into original input. Reconstructed from latent space representation

- Pros: Handle high dimensional data with ease
- Cons: Since, it is deep learning startegy, it will struggle if data is less
- Cons: High computational cost if the depth of network increases

<br>
<br>
<br>

## DEMO
![Problem Demo](https://github.com/ShauryaRawat10/Data-Engineering/blob/8cc8affcf2fabc9db194bf41ab9535b83521bacd/Machine%20Learning/Basics/Storage/ProblemStatement.png)













